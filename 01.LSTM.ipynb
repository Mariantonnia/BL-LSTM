{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tickers = [\"XYL\",\"AWK\",\"SVTRF\",\"UUGWF\",\"WTRG\",\"VEOEF\",\"ECL\",\"GBERF\",\"AQUA\",\"WMS\",\"KTWIF\",\"PNR\",\"TTEK\",\"SBS\",\"AWR\",\"STN\",\"CWT\",\"FELE\",\"WTS\",\"BMI\",\"VMI\",\"PEGRF\",\"FCHRF\",\"SJW\",\"FERG\",\"ROP\",\"DHR\",\"AOS\",\"TTC\",\"IEX\",\"ZWS\",\"MLI\",\"LNN\",\"MWA\",\"ERII\",\"GRC\",\"NWPX\",\"MSEX\",\"ARTNA\",\"YORW\",\"GWRS\",\"WAT\",\"ITRI\",\"IDXX\",\"ACM\",\"A\",\"AQN\",\"FLS\",\"PRMW\",\"CNM\",\"HWKN\",\"MEG\",\"GEBNE.SW\",\"SVT\",\"UU.L\",\"SPX.L\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV file into a pandas DataFrame\n",
    "df = pd.read_csv('water_assets.csv', index_col='Date',decimal=\".\", parse_dates=True)\n",
    "df=df.fillna(method=\"bfill\")\n",
    "df=df.asfreq(\"7d\")\n",
    "tipo_interes = df['interest_rate'].values  # Añadir una variable adicional de tipo de interés"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 30\n",
    "n_out=1\n",
    "\n",
    "param_grid = {\n",
    "    'units1': [64, 128, 256],  # Unidades para la primera capa LSTM\n",
    "    'units2': [32, 64, 128],  # Unidades para la segunda capa LSTM\n",
    "    'units3': [32, 64, 128],  # Unidades para la tercera capa LSTM\n",
    "    'dropout_rate': [0.0, 0.05, 0.1, 0.15, 0.2],  # Tasas de dropout\n",
    "    'batch_size': [16, 32]  # Tamaños de batch\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data, tipo_interes, n_steps, n_out):\n",
    "    \"\"\"\n",
    "    Prepara datos para una red LSTM con múltiples entradas (incluyendo el tipo de interés).\n",
    "    \n",
    "    Args:\n",
    "    - data: matriz numpy con los valores de la serie temporal principal.\n",
    "    - tipo_interes: matriz numpy con los valores del tipo de interés (otra serie temporal).\n",
    "    - n_steps: número de pasos en la secuencia.\n",
    "    - n_out: número de pasos de predicción.\n",
    "    \n",
    "    Returns:\n",
    "    - X: matriz numpy con las características de entrada, que incluye data y tipo_interes.\n",
    "    - y: matriz numpy con las etiquetas de salida.\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - (n_steps + n_out) + 1):\n",
    "        # Usar `data` y `tipo_interes` como entradas\n",
    "        x_data = data[i:(i + n_steps), 0]  # Serie principal\n",
    "        x_interest = tipo_interes[i:(i + n_steps)]  # Tipo de interés\n",
    "        X.append(np.column_stack((x_data, x_interest)))  # Combinar ambas características\n",
    "        y.append(data[i + n_steps:i + n_steps + n_out, 0])  # Etiquetas de salida\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "for i in tickers:\n",
    "    print(\".....................................................\")\n",
    "    print(i)\n",
    "    df_i = df[[i]]\n",
    "    interest = tipo_interes.reshape(-1, 1)  # Asegurar que tipo_interes tenga la misma forma\n",
    "    combined_data = np.hstack((df_i.values, interest))  # Combinar datos con tipo de interés\n",
    "\n",
    "    # Preparar los datos\n",
    "    X, y = prepare_data(combined_data, interest, n_steps, n_out)\n",
    "\n",
    "    # Dividir los datos en entrenamiento, validación y prueba\n",
    "    X_train = X[:int(X.shape[0] * 0.8)]\n",
    "    X_test = X[int(X.shape[0] * 0.8):int(X.shape[0] * 0.9)]\n",
    "    X_val = X[int(X.shape[0] * 0.9):]\n",
    "    \n",
    "    y_train = y[:int(y.shape[0] * 0.8)]\n",
    "    y_test = y[int(y.shape[0] * 0.8):int(y.shape[0] * 0.9)]\n",
    "    y_val = y[int(y.shape[0] * 0.9):]\n",
    "\n",
    "    # Aplicar Min-Max Scaling después del split\n",
    "    scaler_X = MinMaxScaler()\n",
    "    n_features = X_train.shape[2]  # Número de características\n",
    "    X_train = scaler_X.fit_transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
    "    X_test = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
    "    X_val = scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)\n",
    "    \n",
    "    scaler_y = MinMaxScaler()\n",
    "    y_train = scaler_y.fit_transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "    y_test = scaler_y.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "    y_val = scaler_y.transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "\n",
    "    # Definir la arquitectura base del modelo\n",
    "    def create_model(units1, units2, units3, dropout_rate, n_out = 1, n_steps = 30, n_features = 2):\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(units=units1, return_sequences=True, input_shape=(n_steps, n_features)))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(units=units2, return_sequences=True))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(LSTM(units=units3))\n",
    "        model.add(Dropout(dropout_rate))\n",
    "        model.add(Dense(units=n_out))\n",
    "        model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "        return model\n",
    "\n",
    "    # Realizar Grid Search\n",
    "    best_model = None\n",
    "    best_score = float('inf')\n",
    "    \n",
    "    for units1 in param_grid['units1']:\n",
    "        for units2 in param_grid['units2']:\n",
    "            for units3 in param_grid['units3']:\n",
    "                for dropout_rate in param_grid['dropout_rate']:\n",
    "                    for batch_size in param_grid['batch_size']:\n",
    "                        print(f'Tuning: units1={units1}, units2={units2}, units3={units3}, dropout_rate={dropout_rate}, batch_size={batch_size}')\n",
    "                        model = create_model(units1, units2, units3, dropout_rate)\n",
    "                        \n",
    "                        # Ajustar el modelo\n",
    "                        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=100, batch_size=batch_size, verbose=0)\n",
    "                        \n",
    "                        # Evaluar el modelo\n",
    "                        score = model.evaluate(X_test, y_test, verbose=0)\n",
    "                        if score < best_score:\n",
    "                            best_score = score\n",
    "                            best_model = model\n",
    "\n",
    "    # Guardar el mejor modelo encontrado\n",
    "    model = best_model\n",
    "    model.save(f'{i}_best_model.h5')\n",
    "\n",
    "    # Guardar los scalers\n",
    "    joblib.dump(scaler_X, f'{i}_scaler_X.pkl')\n",
    "    joblib.dump(scaler_y, f'{i}_scaler_y.pkl')\n",
    "\n",
    "    print(f\"Best model and scalers for {i} saved as {i}_best_model.h5, {i}_scaler_X.pkl, and {i}_scaler_y.pkl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errores = {}\n",
    "predicciones = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import joblib\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "for i in tickers:\n",
    "    print(\".....................................................\")\n",
    "    print(f\"Procesando predicciones para {i}\")\n",
    "\n",
    "    # Cargar el modelo y los escaladores guardados\n",
    "    model = keras.models.load_model(f'{i}_best_model.h5')\n",
    "    scaler_X = joblib.load(f'{i}_scaler_X.pkl')\n",
    "    scaler_y = joblib.load(f'{i}_scaler_y.pkl')\n",
    "\n",
    "    # Preparar los datos de entrada\n",
    "    df_i = df[[i]]\n",
    "    interest = tipo_interes.reshape(-1, 1)  # Asegurar que tipo_interes tenga la misma forma\n",
    "    combined_data = np.hstack((df_i.values, interest))  # Combinar datos con tipo de interés\n",
    "\n",
    "    # Preparar los datos de entrada y salida\n",
    "    X, y = prepare_data(combined_data, interest, n_steps, n_out)\n",
    "\n",
    "    # Dividir los datos en entrenamiento, validación y prueba\n",
    "    X_train = X[:int(X.shape[0] * 0.8)]\n",
    "    X_test = X[int(X.shape[0] * 0.8):int(X.shape[0] * 0.9)]\n",
    "    X_val = X[int(X.shape[0] * 0.9):]\n",
    "    \n",
    "    y_train = y[:int(y.shape[0] * 0.8)]\n",
    "    y_test = y[int(y.shape[0] * 0.8):int(y.shape[0] * 0.9)]\n",
    "    y_val = y[int(y.shape[0] * 0.9):]\n",
    "\n",
    "    # Escalar los datos utilizando los scalers cargados\n",
    "    n_features = X_train.shape[2]\n",
    "    X_train = scaler_X.transform(X_train.reshape(-1, n_features)).reshape(X_train.shape)\n",
    "    X_test = scaler_X.transform(X_test.reshape(-1, n_features)).reshape(X_test.shape)\n",
    "    X_val = scaler_X.transform(X_val.reshape(-1, n_features)).reshape(X_val.shape)\n",
    "    \n",
    "    y_train = scaler_y.transform(y_train.reshape(-1, 1)).reshape(y_train.shape)\n",
    "    y_test = scaler_y.transform(y_test.reshape(-1, 1)).reshape(y_test.shape)\n",
    "    y_val = scaler_y.transform(y_val.reshape(-1, 1)).reshape(y_val.shape)\n",
    "\n",
    "    # Realizar predicciones\n",
    "    y_pred_val = model.predict(X_val)\n",
    "    y_pred_val = scaler_y.inverse_transform(y_pred_val)  # Desescalar predicciones\n",
    "    y_val = scaler_y.inverse_transform(y_val)  # Desescalar valores reales\n",
    "\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    y_pred_train = scaler_y.inverse_transform(y_pred_train)\n",
    "    y_train = scaler_y.inverse_transform(y_train)\n",
    "\n",
    "    y_pred_test = model.predict(X_test)\n",
    "    y_pred_test = scaler_y.inverse_transform(y_pred_test)\n",
    "    y_test = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "    # Guardar las predicciones y errores\n",
    "    predicciones[f'pred_{i}'] = y_pred_val.flatten()\n",
    "    errores[f'{i}'] = mean_absolute_error(y_pred_val.flatten(), y_val.flatten())\n",
    "\n",
    "    print(f\"Error MAE para {i}: {errores[f'{i}']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir las predicciones a un DataFrame\n",
    "predicciones_df = pd.DataFrame(predicciones)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
